{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb9b54b-ca2e-40b7-bd23-c7130a7631e0",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b207a2a1-24d8-4855-8fe4-36c1ea347ae7",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use conditional probability.\n",
    "\n",
    "Let's denote:\n",
    "\n",
    "A: Event that an employee uses the health insurance plan.\n",
    "B: Event that an employee is a smoker.\n",
    "We want to find P(B|A), the probability of an employee being a smoker given that he/she uses the health insurance plan.\n",
    "\n",
    "We are given:\n",
    "\n",
    "P(A) = 0.70 (Probability that an employee uses the health insurance plan)\n",
    "P(B|A) = 0.40 (Probability that an employee is a smoker given that he/she uses the health insurance plan)\n",
    "\n",
    "\n",
    "We can rearrange the formula to solve for \n",
    "\n",
    "P(A∩B)=P(B∣A)×P(A)\n",
    "\n",
    "Now we can substitute the given values:\n",
    "\n",
    "P(A∩B)=0.40×0.70=0.28\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.28, or 28%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758875fb-2865-4ad2-9e9c-c857dde36f23",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2c8bc-e36f-4db6-ad21-c2a56460bfa0",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes:\n",
    "\n",
    "In Bernoulli Naive Bayes, features are binary variables (0 or 1), representing whether a particular feature is present or absent in a document or sample.\n",
    "It assumes that the features are generated from independent Bernoulli distributions.\n",
    "It's commonly used in text classification tasks where the presence or absence of a word in a document matters more than its frequency.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "In Multinomial Naive Bayes, features represent the frequencies with which certain events have been generated by a multinomial distribution.\n",
    "It's typically used when the features (e.g., word counts) represent the occurrence counts of words or other items within the document.\n",
    "It's widely used in text classification tasks, especially when the frequency of words matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85fce2f-b6c0-4f15-8a25-cd5f9d411b2c",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9a960b-cbdf-4b5a-92c5-fc920d2f41fd",
   "metadata": {},
   "source": [
    " Bernoulli Naive Bayes, missing values can be handled in different ways, depending on the implementation and the specific requirements of the problem. Here are some common approaches:\n",
    "\n",
    "Imputation: Missing values can be replaced with a specific value, such as 0 or 1, depending on whether the feature is binary or categorical. This approach assumes that missing values are indicative of the feature being absent.\n",
    "\n",
    "Mean or Mode Imputation: For binary features, missing values can be imputed with the mode of the feature (i.e., the most common value) across the dataset. This approach assumes that missing values are more likely to be similar to the majority class.\n",
    "\n",
    "Ignoring Missing Values: In some implementations, missing values may simply be ignored during training and prediction. This approach works well if missing values are relatively rare and do not significantly affect the overall performance of the classifier.\n",
    "\n",
    "Explicit Handling: Some implementations of Bernoulli Naive Bayes may include explicit handling of missing values, treating them as a separate category or introducing a separate parameter to model the probability of missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6afe76-355c-417a-802f-65d0b67a405a",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94adeb31-aa0e-454a-9bf0-043564d10d98",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that assumes that the features follow a Gaussian (normal) distribution. It's particularly useful when dealing with continuous features.\n",
    "\n",
    "For multi-class classification, Gaussian Naive Bayes can be adapted straightforwardly by applying the Bayes' theorem to estimate the probability of each class given the input features and then selecting the class with the highest probability as the predicted class.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "Model Training: During training, Gaussian Naive Bayes estimates the parameters (mean and variance) of the Gaussian distribution for each class based on the training data. For each feature in each class, it computes the mean and variance of the feature values.\n",
    "\n",
    "Prediction: To predict the class label for a new instance, Gaussian Naive Bayes calculates the likelihood of the observed feature values under each class's Gaussian distribution. It then combines these likelihoods with the prior probabilities of the classes to compute the posterior probability of each class given the input features using Bayes' theorem. Finally, it selects the class with the highest posterior probability as the predicted class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e984f54-9a81-47a8-a9c3-1bed9c29aa73",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3d6ba05-555c-4e15-8d99-4653951a84cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Metrics:\n",
      "Accuracy: 0.8839380364047911\n",
      "Precision: 0.8869617393737383\n",
      "Recall: 0.8152389047416673\n",
      "F1 score: 0.8481249015095276\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes Metrics:\n",
      "Accuracy: 0.7863496180326323\n",
      "Precision: 0.7393175533565436\n",
      "Recall: 0.7214983911116508\n",
      "F1 score: 0.7282909724016348\n",
      "\n",
      "\n",
      "Gaussian Naive Bayes Metrics:\n",
      "Accuracy: 0.8217730830896915\n",
      "Precision: 0.7103733928118492\n",
      "Recall: 0.9569516119239877\n",
      "F1 score: 0.8130660909542995\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "url = \"spambase.data\"\n",
    "names = [f\"attr_{i}\" for i in range(57)] + ['is_spam']\n",
    "data = pd.read_csv(url, names=names)\n",
    "\n",
    "# Step 3: Split features and target\n",
    "X = data.drop('is_spam', axis=1)\n",
    "y = data['is_spam']\n",
    "\n",
    "# Step 4: Initialize classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Step 5: Evaluate performance using 10-fold cross-validation\n",
    "scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "bernoulli_scores = cross_validate(bernoulli_nb, X, y, cv=10, scoring=scoring)\n",
    "multinomial_scores = cross_validate(multinomial_nb, X, y, cv=10, scoring=scoring)\n",
    "gaussian_scores = cross_validate(gaussian_nb, X, y, cv=10, scoring=scoring)\n",
    "\n",
    "# Step 6: Report performance metrics\n",
    "print(\"Bernoulli Naive Bayes Metrics:\")\n",
    "print(\"Accuracy:\", np.mean(bernoulli_scores['test_accuracy']))\n",
    "print(\"Precision:\", np.mean(bernoulli_scores['test_precision']))\n",
    "print(\"Recall:\", np.mean(bernoulli_scores['test_recall']))\n",
    "print(\"F1 score:\", np.mean(bernoulli_scores['test_f1']))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Multinomial Naive Bayes Metrics:\")\n",
    "print(\"Accuracy:\", np.mean(multinomial_scores['test_accuracy']))\n",
    "print(\"Precision:\", np.mean(multinomial_scores['test_precision']))\n",
    "print(\"Recall:\", np.mean(multinomial_scores['test_recall']))\n",
    "print(\"F1 score:\", np.mean(multinomial_scores['test_f1']))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Gaussian Naive Bayes Metrics:\")\n",
    "print(\"Accuracy:\", np.mean(gaussian_scores['test_accuracy']))\n",
    "print(\"Precision:\", np.mean(gaussian_scores['test_precision']))\n",
    "print(\"Recall:\", np.mean(gaussian_scores['test_recall']))\n",
    "print(\"F1 score:\", np.mean(gaussian_scores['test_f1']))\n",
    "\n",
    "# Step 7: Discussion and conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c94f0-ce8a-4920-8b03-a05c6d9bb4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
