{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d66214-dadb-4df5-a814-1526e2bc1fb9",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e0a891-fbac-455e-9fde-a94e399090dc",
   "metadata": {},
   "source": [
    "f(x)=wâ‹…x+b\n",
    "The goal is to find \n",
    "w\n",
    "w and \n",
    "b such that the margin between the two classes is maximized. Additionally, the SVM aims to minimize the norm of \n",
    "w\n",
    "w to avoid overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ea8a06-f5d1-4d1d-be27-d4a00d9bf892",
   "metadata": {},
   "source": [
    "Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a944d-4a7d-4e86-9a03-4f556e8bdd83",
   "metadata": {},
   "source": [
    "\n",
    "The objective function of a linear Support Vector Machine (SVM) is to find the parameters (weight vector \n",
    "w\n",
    "w and bias term\n",
    "b) that define a hyperplane in such a way that it maximizes the margin between different classes while correctly classifying the training data. The objective function consists of two components: the margin term and a regularization term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61b4462-2e08-4c9a-87a9-1ea5808d1b72",
   "metadata": {},
   "source": [
    "Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0743c553-96b4-4b22-9f2f-72eaf3d3e506",
   "metadata": {},
   "source": [
    "The kernel trick in Support Vector Machines (SVM) is a technique used to implicitly map input features into a higher-dimensional space without explicitly computing the transformed feature vectors. This is achieved by replacing the dot product of feature vectors in the original space with the evaluation of a kernel function. The kernel function calculates the dot product in the higher-dimensional space without explicitly representing the transformed feature vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8b5d28-2fae-45ee-9b87-53c16c2c1672",
   "metadata": {},
   "source": [
    "Q4. What is the role of support vectors in SVM Explain with example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7759158c-5880-4364-ab94-1bcfbb7ab60b",
   "metadata": {},
   "source": [
    "In Support Vector Machines (SVM), support vectors are the data points that are crucial for determining the optimal hyperplane that separates different classes. These are the instances that lie closest to the decision boundary, and they play a key role in defining the margin and the overall performance of the SVM classifier.\n",
    "\n",
    "The main roles of support vectors in SVM are:\n",
    "\n",
    "Determining the Decision Boundary:\n",
    "\n",
    "The decision boundary or hyperplane is chosen in such a way that it maximizes the margin between classes. This margin is defined by the distances from the support vectors to the decision boundary.\n",
    "Support vectors are the data points that lie on the margin or are misclassified, and they influence the position and orientation of the decision boundary.\n",
    "Defining the Margin:\n",
    "\n",
    "The margin is the distance between the decision boundary and the nearest data point from each class.\n",
    "The support vectors are the data points that contribute to the margin, and the optimal hyperplane is the one that maximizes this margin.\n",
    "Handling Non-Linearity (with Kernels):\n",
    "\n",
    "In cases where the data is not linearly separable, support vectors become crucial when using the kernel trick. The kernel function implicitly maps the data into a higher-dimensional space.\n",
    "Support vectors retain their importance even in the higher-dimensional space, and the decision boundary is still determined by a subset of these vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8415ede9-da93-4c42-a980-57674dc39ea0",
   "metadata": {},
   "source": [
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in\n",
    "SVM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02133478-3c58-431e-9c03-0a161944a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1, 2], [2, 3], [2, 5], [3, 2], [4, 5], [5, 2],\n",
    "              [6, 3], [6, 5], [7, 2], [8, 5], [2, 1], [3, 1],\n",
    "              [3, 3], [4, 1], [5, 3], [6, 1], [7, 3], [7, 1],\n",
    "              [8, 3], [9, 1]])\n",
    "y = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
    "\n",
    "# Fit the SVM model\n",
    "clf_hard_margin = svm.SVC(kernel='linear', C=1e10)  # Hard margin SVM\n",
    "clf_hard_margin.fit(X, y)\n",
    "\n",
    "clf_soft_margin = svm.SVC(kernel='linear', C=0.1)  # Soft margin SVM with smaller C\n",
    "clf_soft_margin.fit(X, y)\n",
    "\n",
    "# Create a mesh grid for plotting\n",
    "xx, yy = np.meshgrid(np.linspace(0, 10, 100), np.linspace(0, 7, 100))\n",
    "\n",
    "# Plotting function\n",
    "def plot_svm(clf, title):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot data points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired, edgecolors='k', marker='o')\n",
    "\n",
    "    # Plot the decision boundary\n",
    "    Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "\n",
    "    # Plot support vectors\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100, facecolors='none', edgecolors='k')\n",
    "\n",
    "    # Set plot labels and title\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(title)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# Plot Hard Margin SVM\n",
    "plot_svm(clf_hard_margin, 'Hard Margin SVM')\n",
    "\n",
    "# Plot Soft Margin SVM\n",
    "plot_svm(clf_soft_margin, 'Soft Margin SVM (C=0.1)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b29a55-2473-4a79-a2e3-a51bd4b82b21",
   "metadata": {},
   "source": [
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d9e11f-e08d-4cab-afa3-949989f5282c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Use only two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to plot decision boundaries\n",
    "def plot_decision_boundary(X, y, model, title):\n",
    "    h = .02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, edgecolors='k', marker='o')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "# Train a linear SVM classifier using scikit-learn\n",
    "clf_sklearn = SVC(kernel='linear', C=1)\n",
    "clf_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred_sklearn = clf_sklearn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy using scikit-learn implementation\n",
    "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
    "print(f\"Accuracy (scikit-learn): {accuracy_sklearn:.2f}\")\n",
    "\n",
    "# Plot decision boundary using scikit-learn implementation\n",
    "plot_decision_boundary(X, y, clf_sklearn, 'Linear SVM (scikit-learn)')\n",
    "\n",
    "# Implement linear SVM from scratch\n",
    "class LinearSVM:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, C=1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.C = C\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "        self.W = np.zeros(n + 1)\n",
    "        X_augmented = np.c_[np.ones(m), X]\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            hinge_loss = 1 - y * (X_augmented.dot(self.W))\n",
    "            gradient = -2 * self.C * np.dot(X_augmented.T, hinge_loss * (hinge_loss > 0) * y) / m\n",
    "            self.W -= self.learning_rate * gradient\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_augmented = np.c_[np.ones(X.shape[0]), X]\n",
    "        return np.sign(X_augmented.dot(self.W))\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8490c00-eab3-46ad-ad3d-086645f5ea4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad2028-7061-4ddb-b9b0-25d347d79414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985d09af-4682-4af3-a5f6-5ee695baf32c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
