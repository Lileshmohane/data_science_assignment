{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65c73ed-5b81-4c28-80ff-84da549e7f40",
   "metadata": {},
   "source": [
    "1. Explain the core components of the Hadoop ecosystem and their respective roles in processing and\n",
    "storing big data. Provide a brief overview of HDFS, MapReduce, and YARN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548647bc-cbe6-4847-af72-249475ea149f",
   "metadata": {},
   "source": [
    "The Hadoop ecosystem is a collection of open-source software tools and frameworks designed for distributed storage and processing of large datasets. The core components of the Hadoop ecosystem include Hadoop Distributed File System (HDFS), MapReduce, and Yet Another Resource Negotiator (YARN).\n",
    "\n",
    "Hadoop Distributed File System (HDFS):\n",
    "\n",
    "Role: HDFS is the primary storage system of Hadoop, designed to store and manage large volumes of data reliably and efficiently across a distributed cluster of commodity hardware.\n",
    "Overview: HDFS breaks large data files into smaller blocks (typically 128 MB or 256 MB in size) and distributes these blocks across multiple nodes in the Hadoop cluster. It also replicates these blocks across multiple nodes for fault tolerance. This distributed and replicated storage approach ensures that even if some nodes in the cluster fail, the data is still accessible and can be processed.\n",
    "MapReduce:\n",
    "\n",
    "Role: MapReduce is a programming model and processing engine for distributed computing of large data sets. It allows developers to write parallel processing applications without worrying about the underlying details of distributed computing.\n",
    "Overview: MapReduce processes data in two main stages - the Map phase and the Reduce phase. The Map phase involves breaking down the input data into key-value pairs, and the Reduce phase aggregates and processes these pairs to produce the final output. MapReduce automatically handles parallelization, fault tolerance, and load balancing, making it suitable for distributed processing of large datasets.\n",
    "Yet Another Resource Negotiator (YARN):\n",
    "\n",
    "Role: YARN is the resource management layer of Hadoop, responsible for managing and allocating resources in a Hadoop cluster. It allows multiple applications to share resources and ensures efficient utilization of cluster resources.\n",
    "Overview: YARN decouples the resource management and job scheduling/monitoring functions of the MapReduce engine. This separation allows various processing frameworks, not just MapReduce, to run on the same Hadoop cluster. YARN manages resources (CPU and memory) and schedules jobs from different applications, making the cluster more versatile and suitable for a broader range of data processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ea649-e0da-4b22-ae39-11b95f9dbb9e",
   "metadata": {},
   "source": [
    "2. Discuss the Hadoop Distributed File System (HDFS) in detail. Explain how it stores and manages data in a\n",
    "distributed environment. Describe the key concepts of HDFS, such as NameNode, DataNode, and blocks, and\n",
    "how they contribute to data reliability and fault tolerance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bac05-771a-458b-8f39-2d7afca379aa",
   "metadata": {},
   "source": [
    "Hadoop Distributed File System (HDFS):\n",
    "\n",
    "HDFS is the distributed file storage system that forms the core of the Hadoop ecosystem. It is designed to store and manage large datasets across a distributed cluster of machines. Below are key concepts and components of HDFS that contribute to its functioning in a distributed environment:\n",
    "\n",
    "NameNode:\n",
    "\n",
    "The NameNode is a critical component of HDFS and serves as the master server that manages the metadata and namespace of the file system.\n",
    "It keeps track of the structure of the file system, including information about files, directories, and their hierarchical organization.\n",
    "The metadata includes the locations of data blocks, file permissions, and modification times.\n",
    "DataNode:\n",
    "\n",
    "DataNodes are responsible for storing the actual data in HDFS. They are distributed across the cluster and store data in the form of blocks.\n",
    "Each DataNode manages the storage attached to the machine it runs on and periodically sends heartbeat signals to the NameNode to confirm its liveliness.\n",
    "DataNodes also report block information to the NameNode and participate in block replication to ensure fault tolerance.\n",
    "Blocks:\n",
    "\n",
    "HDFS divides large files into fixed-size blocks (typically 128 MB or 256 MB). This block size is configurable and chosen based on the characteristics of the data and the storage infrastructure.\n",
    "Each block is stored independently and can be replicated across multiple DataNodes for fault tolerance. The default replication factor is usually three, meaning each block has two additional copies stored on different nodes.\n",
    "The block size and replication contribute to the fault tolerance and parallel processing capabilities of HDFS. If a DataNode fails or becomes unreachable, the system can still access the data from the replicas stored on other nodes.\n",
    "Replication:\n",
    "\n",
    "Replication is a key feature of HDFS that enhances data reliability and fault tolerance. Each block is replicated across multiple DataNodes to ensure that even if one or more nodes fail, the data remains accessible.\n",
    "The replication factor is configurable, but the default is often set to three. This means that each block is stored on three different DataNodes.\n",
    "Write Pipeline:\n",
    "\n",
    "When a client wants to write a file to HDFS, the data is divided into blocks, and the blocks are sequentially written to the DataNodes in a pipeline fashion.\n",
    "The client first communicates with the NameNode to get the locations of the target DataNodes. It then writes the data to the first DataNode in the pipeline, which, in turn, replicates the data to the next node, and so on.\n",
    "Read Process:\n",
    "\n",
    "Reading involves the reverse process. The client communicates with the NameNode to get the locations of the DataNodes hosting the required blocks. It then reads the data from the nearest DataNode, reducing network overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01112480-0206-4ade-89fd-ac376716e7e5",
   "metadata": {},
   "source": [
    "Hadoop Distributed File System (HDFS):\n",
    "\n",
    "HDFS is the distributed file storage system that forms the core of the Hadoop ecosystem. It is designed to store and manage large datasets across a distributed cluster of machines. Below are key concepts and components of HDFS that contribute to its functioning in a distributed environment:\n",
    "\n",
    "NameNode:\n",
    "\n",
    "The NameNode is a critical component of HDFS and serves as the master server that manages the metadata and namespace of the file system.\n",
    "It keeps track of the structure of the file system, including information about files, directories, and their hierarchical organization.\n",
    "The metadata includes the locations of data blocks, file permissions, and modification times.\n",
    "DataNode:\n",
    "\n",
    "DataNodes are responsible for storing the actual data in HDFS. They are distributed across the cluster and store data in the form of blocks.\n",
    "Each DataNode manages the storage attached to the machine it runs on and periodically sends heartbeat signals to the NameNode to confirm its liveliness.\n",
    "DataNodes also report block information to the NameNode and participate in block replication to ensure fault tolerance.\n",
    "Blocks:\n",
    "\n",
    "HDFS divides large files into fixed-size blocks (typically 128 MB or 256 MB). This block size is configurable and chosen based on the characteristics of the data and the storage infrastructure.\n",
    "Each block is stored independently and can be replicated across multiple DataNodes for fault tolerance. The default replication factor is usually three, meaning each block has two additional copies stored on different nodes.\n",
    "The block size and replication contribute to the fault tolerance and parallel processing capabilities of HDFS. If a DataNode fails or becomes unreachable, the system can still access the data from the replicas stored on other nodes.\n",
    "Replication:\n",
    "\n",
    "Replication is a key feature of HDFS that enhances data reliability and fault tolerance. Each block is replicated across multiple DataNodes to ensure that even if one or more nodes fail, the data remains accessible.\n",
    "The replication factor is configurable, but the default is often set to three. This means that each block is stored on three different DataNodes.\n",
    "Write Pipeline:\n",
    "\n",
    "When a client wants to write a file to HDFS, the data is divided into blocks, and the blocks are sequentially written to the DataNodes in a pipeline fashion.\n",
    "The client first communicates with the NameNode to get the locations of the target DataNodes. It then writes the data to the first DataNode in the pipeline, which, in turn, replicates the data to the next node, and so on.\n",
    "Read Process:\n",
    "\n",
    "Reading involves the reverse process. The client communicates with the NameNode to get the locations of the DataNodes hosting the required blocks. It then reads the data from the nearest DataNode, reducing network overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2795027b-f15f-4d74-ac38-842bd2eb138d",
   "metadata": {},
   "source": [
    "3. Write a step-by-step explanation of how the MapReduce framework works. Use a real-world example to\n",
    "illustrate the Map and Reduce phases. Discuss the advantages and limitations of MapReduce for processing\n",
    "large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661c1da9-73b6-46ac-95e1-32480ece779e",
   "metadata": {},
   "source": [
    "Step-by-Step Explanation of MapReduce Framework:\n",
    "\n",
    "Input Splitting:\n",
    "\n",
    "The input data, usually a large dataset, is divided into smaller chunks called input splits.\n",
    "Each input split is processed independently by a separate map task.\n",
    "Map Phase:\n",
    "\n",
    "Map Task Execution:\n",
    "\n",
    "Each map task processes an input split and produces a set of intermediate key-value pairs.\n",
    "The map function is user-defined and specified by the developer to extract relevant information from the input data.\n",
    "Shuffling and Sorting:\n",
    "\n",
    "The intermediate key-value pairs from all map tasks are shuffled and sorted based on the keys.\n",
    "This ensures that all values associated with a particular key are grouped together.\n",
    "Reduce Phase:\n",
    "\n",
    "Reduce Task Execution:\n",
    "\n",
    "Each reduce task receives a subset of the shuffled and sorted key-value pairs.\n",
    "The reduce function is user-defined and specified by the developer to aggregate, filter, or transform the data.\n",
    "Output Generation:\n",
    "\n",
    "The reduce tasks produce the final output by processing the intermediate key-value pairs and generating the output key-value pairs.\n",
    "Final Output:\n",
    "\n",
    "The final output consists of key-value pairs produced by the reduce tasks, representing the result of the MapReduce job.\n",
    "Real-World Example: Word Count\n",
    "\n",
    "Let's consider the classic Word Count example to illustrate the MapReduce framework:\n",
    "\n",
    "Map Phase:\n",
    "\n",
    "Input: \"Hello world, this is a word count example. Hello world.\"\n",
    "Map Output:\n",
    "(Hello, 1), (world, 1), (this, 1), (is, 1), (a, 1), (word, 1), (count, 1), (example, 1), (Hello, 1), (world, 1)\n",
    "Shuffling and Sorting:\n",
    "\n",
    "The intermediate key-value pairs are sorted based on the keys:\n",
    "(Hello, [1, 1]), (a, [1]), (count, [1]), (example, [1]), (is, [1]), (this, [1]), (word, [1, 1]), (world, [1, 1])\n",
    "Reduce Phase:\n",
    "\n",
    "Reduce Output:\n",
    "(Hello, 2), (a, 1), (count, 1), (example, 1), (is, 1), (this, 1), (word, 2), (world, 2)\n",
    "Advantages of MapReduce:\n",
    "\n",
    "Scalability:\n",
    "\n",
    "MapReduce is highly scalable, allowing it to process large datasets by distributing the workload across multiple nodes in a cluster.\n",
    "Fault Tolerance:\n",
    "\n",
    "MapReduce provides fault tolerance by replicating data across nodes. If a node fails, the processing tasks are automatically reassigned to other nodes.\n",
    "Simplicity:\n",
    "\n",
    "The programming model is relatively simple, especially for developers familiar with functional programming concepts, making it accessible for a wide range of users.\n",
    "Limitations of MapReduce:\n",
    "\n",
    "Latency:\n",
    "\n",
    "MapReduce may introduce latency due to its batch-oriented nature. Real-time processing of data is not its strong suit.\n",
    "Complexity for Some Tasks:\n",
    "\n",
    "While MapReduce is effective for certain types of batch processing tasks, it may not be the best fit for more complex algorithms or iterative machine learning tasks.\n",
    "Data Movement Overhead:\n",
    "\n",
    "The shuffling and sorting phase involves significant data movement between nodes, potentially leading to increased network overhead.\n",
    "Limited Expressiveness:\n",
    "\n",
    "Some data processing tasks may require multiple MapReduce jobs, as the framework has limitations in expressing complex algorithms in a single job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f24cb-06a2-43f1-a45a-e8045ac3d17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aba65f6e-8f1c-45b8-b6b4-53b776964126",
   "metadata": {},
   "source": [
    "4. Explore the role of YARN in Hadoop. Explain how it manages cluster resources and schedules applications.\n",
    "Compare YARN with the earlier Hadoop 1.x architecture and highlight the benefits of YARN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271cb5e2-4430-4f84-a302-fb82801a1cc7",
   "metadata": {},
   "source": [
    "\n",
    "Yet Another Resource Negotiator (YARN):\n",
    "\n",
    "YARN is a key component of the Hadoop ecosystem introduced in Hadoop 2.x to address limitations in the original Hadoop 1.x architecture. YARN separates the resource management and job scheduling functions, providing a more flexible and efficient framework for running diverse workloads on a Hadoop cluster.\n",
    "\n",
    "Role of YARN:\n",
    "\n",
    "Resource Management:\n",
    "\n",
    "YARN manages and allocates resources (CPU and memory) across applications running on a Hadoop cluster.\n",
    "It allows multiple applications to share resources on the same cluster, enabling a more efficient and versatile use of computing resources.\n",
    "Job Scheduling:\n",
    "\n",
    "YARN includes a ResourceManager that oversees the allocation of resources and a NodeManager on each node that manages resources locally.\n",
    "ResourceManager is responsible for scheduling applications, negotiating resources with NodeManagers, and monitoring their execution.\n",
    "NodeManager is responsible for managing resources on a specific node and reporting to the ResourceManager.\n",
    "Application Master:\n",
    "\n",
    "YARN introduces the concept of an Application Master, which is a per-application framework that negotiates resources from the ResourceManager and works with NodeManagers to execute and monitor tasks.\n",
    "Each application running on YARN has its own Application Master, which is responsible for managing resources for that specific application.\n",
    "Flexibility:\n",
    "\n",
    "YARN supports multiple processing models beyond MapReduce, allowing other distributed computing frameworks to run on the same cluster. This includes interactive query processing (Apache Tez), real-time data processing (Apache Storm), and distributed data processing (Apache Spark).\n",
    "Comparison with Hadoop 1.x Architecture:\n",
    "\n",
    "In the earlier Hadoop 1.x architecture, there was a single JobTracker that performed both resource management and job scheduling for MapReduce jobs. This tight integration limited the system's flexibility and scalability, particularly when it came to running multiple types of workloads simultaneously.\n",
    "\n",
    "Benefits of YARN:\n",
    "\n",
    "Multi-Tenancy:\n",
    "\n",
    "YARN supports multi-tenancy, allowing multiple applications to share the same cluster efficiently. This is crucial for organizations with diverse workloads and teams that need to run various types of applications simultaneously.\n",
    "Flexibility and Extensibility:\n",
    "\n",
    "YARN's architecture is more flexible and extensible, enabling the integration of various processing engines beyond MapReduce. This flexibility allows organizations to choose the right tool for the right job, making Hadoop a more versatile and powerful platform.\n",
    "Improved Resource Utilization:\n",
    "\n",
    "YARN allows for better resource utilization by efficiently managing resources across applications. It dynamically allocates resources based on the specific needs of each application, leading to improved cluster utilization.\n",
    "Enhanced Scalability:\n",
    "\n",
    "YARN provides better scalability by allowing the cluster to scale horizontally to meet increasing demands. With the ability to run various applications concurrently, YARN supports larger and more diverse workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f290485c-dcad-45f2-ba68-66bfd33cf995",
   "metadata": {},
   "source": [
    "5. Provide an overview of some popular components within the Hadoop ecosystem, such as HBase, Hive, Pig,\n",
    "and Spark. Describe the use cases and differences between these components. Choose one component and\n",
    "explain how it can be integrated into a Hadoop ecosystem for specific data processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0d0645-08a9-4886-ae14-1e8b41fd7ca5",
   "metadata": {},
   "source": [
    "HBase:\n",
    "\n",
    "Use Case: HBase is a NoSQL database that provides real-time, random read and write access to large amounts of data. It is suitable for applications requiring low-latency access to scalable and distributed data, such as those in the financial sector or Internet of Things (IoT) applications.\n",
    "Hive:\n",
    "\n",
    "Use Case: Hive is a data warehousing and SQL-like query language system built on top of Hadoop. It enables users to query and analyze large datasets using a familiar SQL syntax. Hive is commonly used for batch processing and is suitable for business intelligence and reporting applications.\n",
    "Pig:\n",
    "\n",
    "Use Case: Apache Pig is a high-level scripting language designed for processing and analyzing large datasets. It simplifies the development of complex data transformations, and it is often used for ETL (Extract, Transform, Load) tasks and data preparation in a Hadoop ecosystem.\n",
    "Spark:\n",
    "\n",
    "Use Case: Apache Spark is a fast and general-purpose data processing engine that supports batch processing, stream processing, machine learning, and graph processing. It provides in-memory processing capabilities, making it faster than traditional MapReduce for certain workloads. Spark is versatile and suitable for a wide range of data processing tasks.\n",
    "Integration Example: Apache Spark in Hadoop Ecosystem\n",
    "\n",
    "Use Case: Let's consider a scenario where real-time data analytics is required on a large dataset stored in HDFS.\n",
    "\n",
    "Integration Steps:\n",
    "\n",
    "Data Ingestion: The data is stored in HDFS using Hadoop, making it accessible for processing.\n",
    "\n",
    "Spark Application Development: A Spark application is developed using the Spark API, which provides high-level APIs for distributed data processing. Spark supports various programming languages like Scala, Java, and Python.\n",
    "\n",
    "Cluster Execution: The Spark application is submitted to the cluster. Spark utilizes YARN for resource management and can coexist with other Hadoop components.\n",
    "\n",
    "Data Processing: Spark reads the data from HDFS, processes it in-memory, and performs the required analytics or computations. Spark can handle batch processing as well as stream processing, making it suitable for real-time analytics.\n",
    "\n",
    "Output: The results of the Spark job can be stored back in HDFS or used for further analysis and reporting.\n",
    "\n",
    "Differences Between Components:\n",
    "\n",
    "HBase vs. Hive:\n",
    "\n",
    "HBase is a NoSQL database for real-time access to large datasets, while Hive is a data warehousing system for querying and analyzing data using SQL-like queries. HBase is suitable for low-latency applications, while Hive is more appropriate for batch processing and business intelligence.\n",
    "Pig vs. Spark:\n",
    "\n",
    "Pig is a scripting language for data processing, while Spark is a general-purpose data processing engine that supports multiple programming languages. Pig is often used for ETL tasks, while Spark provides a broader range of capabilities, including batch and stream processing, machine learning, and graph processing.\n",
    "Advantages of Spark:\n",
    "\n",
    "In-Memory Processing: Spark performs in-memory processing, which makes it significantly faster than traditional MapReduce for certain workloads.\n",
    "\n",
    "Versatility: Spark supports a wide range of data processing tasks, including batch processing, interactive queries, streaming, machine learning, and graph processing.\n",
    "\n",
    "Ease of Use: Spark provides high-level APIs in multiple programming languages, making it accessible to a broader audience of developers and data scientists.\n",
    "\n",
    "Unified Platform: Spark provides a unified platform for various data processing tasks, eliminating the need for multiple specialized frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0162eb4-5aeb-42ff-bdba-adcc58afcb9a",
   "metadata": {},
   "source": [
    "6. Explain the key differences between Apache Spark and Hadoop MapReduce. How does Spark overcome\n",
    "some of the limitations of MapReduce for big data processing tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b749c-a9b2-4447-b809-e9a653f8ffe0",
   "metadata": {},
   "source": [
    "Processing Model:\n",
    "\n",
    "MapReduce:\n",
    "MapReduce processes data in two stages: the Map phase and the Reduce phase. Each stage writes intermediate results to disk, causing potential performance overhead.\n",
    "Spark:\n",
    "Spark processes data in-memory, reducing the need for intermediate data to be written to disk. It employs a directed acyclic graph (DAG) execution engine that optimizes the entire workflow, enabling iterative and interactive processing.\n",
    "Data Processing Speed:\n",
    "\n",
    "MapReduce:\n",
    "MapReduce processes data in a batch-oriented fashion, which can result in high latency for iterative algorithms or interactive queries.\n",
    "Spark:\n",
    "Spark's in-memory processing significantly improves processing speed, making it suitable for iterative machine learning algorithms and interactive data analysis.\n",
    "Ease of Use:\n",
    "\n",
    "MapReduce:\n",
    "Writing MapReduce programs can be complex, as developers need to handle low-level details of distributed computing, such as managing key-value pairs and intermediate data.\n",
    "Spark:\n",
    "Spark provides high-level APIs in multiple programming languages (Scala, Java, Python, and R), making it more accessible and easier to use for developers. It also supports SQL queries, machine learning libraries, and graph processing libraries.\n",
    "Iterative Processing:\n",
    "\n",
    "MapReduce:\n",
    "Iterative algorithms (common in machine learning) can be inefficient in MapReduce as each iteration involves reading from and writing to HDFS, incurring additional I/O overhead.\n",
    "Spark:\n",
    "Spark's ability to keep intermediate data in memory between iterations makes it well-suited for iterative algorithms, improving performance and reducing the need for repetitive data read/write operations.\n",
    "Data Pipelining:\n",
    "\n",
    "MapReduce:\n",
    "MapReduce requires multiple Map and Reduce jobs, resulting in multiple I/O operations and potential performance bottlenecks.\n",
    "Spark:\n",
    "Spark supports data pipelining, allowing multiple data processing tasks to be executed in a directed acyclic graph (DAG) in a single job. This reduces the overhead of writing intermediate data to disk between tasks.\n",
    "Flexibility:\n",
    "\n",
    "MapReduce:\n",
    "Primarily designed for batch processing, making it less flexible for other types of workloads.\n",
    "Spark:\n",
    "Spark is more versatile, supporting batch processing, interactive queries, stream processing, machine learning, and graph processing within a unified framework.\n",
    "How Spark Overcomes MapReduce Limitations:\n",
    "\n",
    "In-Memory Processing:\n",
    "\n",
    "Spark performs in-memory processing, minimizing the need for data to be written to disk between stages. This significantly improves performance for iterative algorithms and interactive queries.\n",
    "Data Sharing Between Tasks:\n",
    "\n",
    "Spark allows data to be cached in memory between stages, reducing the need to recompute data for multiple tasks. This feature enhances the efficiency of iterative algorithms.\n",
    "Unified Processing Engine:\n",
    "\n",
    "Spark provides a unified platform for various data processing tasks, eliminating the need for different specialized frameworks. This simplifies the development and maintenance of big data applications.\n",
    "Higher-Level Abstractions:\n",
    "\n",
    "Spark provides high-level APIs in multiple programming languages, as well as libraries for SQL queries, machine learning, and graph processing. This makes it more accessible and user-friendly compared to the lower-level abstractions of MapReduce.\n",
    "Improved DAG Execution Model:\n",
    "\n",
    "Spark's directed acyclic graph (DAG) execution model optimizes the entire workflow, allowing for better task coordination and optimization of execution plans. This is especially beneficial for complex workflows and data pipelining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1af594-53b8-4640-b610-e86f03b1a123",
   "metadata": {},
   "source": [
    "7. Write a Spark application in Scala or Python that reads a text file, counts the occurrences of each word,\n",
    "and returns the top 10 most frequent words. Explain the key components and steps involved in this\n",
    "application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04ff90-2033-49f9-9317-62a47e375c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "object WordCount {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    // Create a SparkConf and SparkContext\n",
    "    val conf = new SparkConf().setAppName(\"WordCount\").setMaster(\"local[*]\")\n",
    "    val sc = new SparkContext(conf)\n",
    "\n",
    "    // Load the input text file\n",
    "    val inputRDD = sc.textFile(\"path/to/your/input.txt\")\n",
    "\n",
    "    // Perform word count\n",
    "    val wordCountRDD = inputRDD\n",
    "      .flatMap(line => line.split(\"\\\\s+\"))\n",
    "      .map(word => (word.toLowerCase, 1))\n",
    "      .reduceByKey(_ + _)\n",
    "\n",
    "    // Get the top 10 most frequent words\n",
    "    val topWords = wordCountRDD\n",
    "      .sortBy(pair => pair._2, ascending = false)\n",
    "      .take(10)\n",
    "\n",
    "    // Print the results\n",
    "    println(\"Top 10 most frequent words:\")\n",
    "    topWords.foreach(println)\n",
    "\n",
    "    // Stop the SparkContext\n",
    "    sc.stop()\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b0abe-f8ae-4655-968b-fb4e83418169",
   "metadata": {},
   "source": [
    "SparkConf and SparkContext:\n",
    "\n",
    "SparkConf is used to configure the properties of the Spark application.\n",
    "SparkContext is the entry point to the Spark cluster and is required for creating RDDs (Resilient Distributed Datasets) and performing various operations.\n",
    "Loading Input Data:\n",
    "\n",
    "The textFile method is used to load the content of the input text file into an RDD. In this example, replace \"path/to/your/input.txt\" with the actual path to your input file.\n",
    "Word Count:\n",
    "\n",
    "The flatMap operation is used to split each line into words.\n",
    "The map operation is used to create key-value pairs with words as keys and the count (initialized to 1) as values.\n",
    "The reduceByKey operation is used to aggregate the counts for each word.\n",
    "Top 10 Most Frequent Words:\n",
    "\n",
    "The sortBy operation is applied to sort the word-count pairs based on the count in descending order.\n",
    "The take(10) operation is used to retrieve the top 10 most frequent words.\n",
    "Print Results:\n",
    "\n",
    "The results are printed to the console using the println method.\n",
    "Stop SparkContext:\n",
    "\n",
    "The stop method is called to gracefully stop the SparkContext."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981c7037-bcad-4d80-a108-2e2033c4da10",
   "metadata": {},
   "source": [
    "8. Using Spark RDDs (Resilient Distributed Datasets), perform the following tasks on a dataset of your\n",
    "choice:\n",
    "a. Filter the data to select only rows that meet specific criteria.\n",
    "b. Map a transformation to modify a specific column in the dataset.\n",
    "c. Reduce the dataset to calculate a meaningful aggregation (e.g., sum, average)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de645c5-8420-4f23-8fe4-d48a9cd54706",
   "metadata": {},
   "source": [
    "a. Filter the data to select only rows that meet specific criteria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6bbbb1-d43b-4189-84b4-0c817267ae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "object FilterDataExample {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = new SparkConf().setAppName(\"FilterDataExample\").setMaster(\"local[*]\")\n",
    "    val sc = new SparkContext(conf)\n",
    "\n",
    "    // Load the dataset\n",
    "    val inputRDD = sc.textFile(\"path/to/sales_data.txt\")\n",
    "\n",
    "    // Filter rows where the product is \"apple\"\n",
    "    val filteredRDD = inputRDD.filter(line => line.split(\",\")(0) == \"apple\")\n",
    "\n",
    "    // Print the results\n",
    "    filteredRDD.foreach(println)\n",
    "\n",
    "    // Stop the SparkContext\n",
    "    sc.stop()\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c730e-6a23-4d73-998b-cc421227e35b",
   "metadata": {},
   "source": [
    "Map a transformation to modify a specific column in the dataset:\n",
    "\n",
    "In this example, let's say we want to double the quantity for each row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c85a7e-c6c9-4662-8cb6-a3156e3984ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "object MapTransformationExample {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = new SparkConf().setAppName(\"MapTransformationExample\").setMaster(\"local[*]\")\n",
    "    val sc = new SparkContext(conf)\n",
    "\n",
    "    // Load the dataset\n",
    "    val inputRDD = sc.textFile(\"path/to/sales_data.txt\")\n",
    "\n",
    "    // Map transformation: Double the quantity\n",
    "    val modifiedRDD = inputRDD.map(line => {\n",
    "      val parts = line.split(\",\")\n",
    "      val product = parts(0)\n",
    "      val quantity = parts(1).toInt * 2\n",
    "      val price = parts(2)\n",
    "      s\"$product,$quantity,$price\"\n",
    "    })\n",
    "\n",
    "    // Print the results\n",
    "    modifiedRDD.foreach(println)\n",
    "\n",
    "    // Stop the SparkContext\n",
    "    sc.stop()\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc822c-5806-404b-a061-812040e8917d",
   "metadata": {},
   "source": [
    "Reduce the dataset to calculate a meaningful aggregation (e.g., sum, average):\n",
    "\n",
    "In this example, let's calculate the total revenue by product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c31268-5ae7-4286-9660-a54307b1d7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "\n",
    "object ReduceAggregationExample {\n",
    "  def main(args: Array[String]): Unit = {\n",
    "    val conf = new SparkConf().setAppName(\"ReduceAggregationExample\").setMaster(\"local[*]\")\n",
    "    val sc = new SparkContext(conf)\n",
    "\n",
    "    // Load the dataset\n",
    "    val inputRDD = sc.textFile(\"path/to/sales_data.txt\")\n",
    "\n",
    "    // Map transformation: Extract product and revenue (quantity * price)\n",
    "    val mappedRDD = inputRDD.map(line => {\n",
    "      val parts = line.split(\",\")\n",
    "      val product = parts(0)\n",
    "      val quantity = parts(1).toInt\n",
    "      val price = parts(2).toDouble\n",
    "      (product, quantity * price)\n",
    "    })\n",
    "\n",
    "    // Reduce aggregation: Calculate total revenue by product\n",
    "    val totalRevenueByProduct = mappedRDD.reduceByKey(_ + _)\n",
    "\n",
    "    // Print the results\n",
    "    totalRevenueByProduct.foreach(println)\n",
    "\n",
    "    // Stop the SparkContext\n",
    "    sc.stop()\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece5c7bc-3f96-4516-b48d-4eaee2ed5ce2",
   "metadata": {},
   "source": [
    "9. Create a Spark DataFrame in Python or Scala by loading a dataset (e.g., CSV or JSON) and perform the\n",
    "following operations:\n",
    "a. Select specific columns from the DataFrame.\n",
    "b. Filter rows based on certain conditions.\n",
    "c. Group the data by a particular column and calculate aggregations (e.g., sum, average).\n",
    "d. Join two DataFrames based on a common key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe376753-c058-459a-8d4d-8d71d915bf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
    "\n",
    "# Load the dataset into a DataFrame\n",
    "df = spark.read.csv(\"path/to/sales_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Select specific columns\n",
    "selected_columns = df.select(\"id\", \"product\", \"quantity\")\n",
    "selected_columns.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82c23e0-91e5-49fa-b413-dc89aef747aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "// Create a Spark session\n",
    "val spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
    "\n",
    "// Load the dataset into a DataFrame\n",
    "val df = spark.read.csv(\"path/to/sales_data.csv\").toDF(\"id\", \"product\", \"quantity\", \"price\")\n",
    "\n",
    "// Select specific columns\n",
    "val selectedColumns = df.select(\"id\", \"product\", \"quantity\")\n",
    "selectedColumns.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f53fb-73cc-42f7-b685-781c2330fe79",
   "metadata": {},
   "outputs": [],
   "source": [
    " Filter rows based on certain conditions:\n",
    "\n",
    "Python (PySpark):\n",
    "    \n",
    "    # Filter rows where the quantity is greater than 5\n",
    "filtered_rows = df.filter(df[\"quantity\"] > 5)\n",
    "filtered_rows.show()\n",
    "\n",
    "Scala:\n",
    "\n",
    "// Filter rows where the quantity is greater than 5\n",
    "val filteredRows = df.filter(\"quantity > 5\")\n",
    "filteredRows.show()\n",
    "\n",
    "\n",
    "Group the data by a particular column and calculate aggregations (e.g., sum, average):\n",
    "\n",
    "Python (PySpark):\n",
    "    # Group by the 'product' column and calculate the sum of quantity and average of price\n",
    "grouped_data = df.groupBy(\"product\").agg({\"quantity\": \"sum\", \"price\": \"avg\"})\n",
    "grouped_data.show()\n",
    "\n",
    "// Group by the 'product' column and calculate the sum of quantity and average of price\n",
    "val groupedData = df.groupBy(\"product\").agg(Map(\"quantity\" -> \"sum\", \"price\" -> \"avg\"))\n",
    "groupedData.show()\n",
    "\n",
    "\n",
    "# Create a second DataFrame for demonstration purposes\n",
    "df2 = spark.read.csv(\"path/to/other_data.csv\", header=True, inferSchema=True).toDF(\"id\", \"description\")\n",
    "\n",
    "# Join the two DataFrames on the 'id' column\n",
    "joined_df = df.join(df2, \"id\")\n",
    "joined_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be3b667-6fda-49ac-904c-8b026d279f49",
   "metadata": {},
   "source": [
    "10. Set up a Spark Streaming application to process real-time data from a source (e.g., Apache Kafka or a\n",
    "simulated data source). The application should:\n",
    "a. Ingest data in micro-batches.\n",
    "b. Apply a transformation to the streaming data (e.g., filtering, aggregation).\n",
    "c. Output the processed data to a sink (e.g., write to a file, a database, or display it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3eb864a-c6fc-4dc7-b5b7-2f5d94ee4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ingest data in micro-batches:\n",
    "\n",
    "Python (PySpark):\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"StreamingExample\").getOrCreate()\n",
    "\n",
    "# Create a StreamingContext with a batch interval of 5 seconds\n",
    "ssc = StreamingContext(spark.sparkContext, batchDuration=5)\n",
    "\n",
    "# Define Kafka parameters\n",
    "kafka_params = {\"bootstrap.servers\": \"your_kafka_broker\", \"key.deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\", \"value.deserializer\": \"org.apache.kafka.common.serialization.StringDeserializer\", \"group.id\": \"your_group_id\"}\n",
    "\n",
    "# Create a DStream that represents the stream of data from Kafka (topic: \"your_topic\")\n",
    "stream = KafkaUtils.createDirectStream(ssc, topics=[\"your_topic\"], kafkaParams=kafka_params)\n",
    "\n",
    "# Process each micro-batch\n",
    "lines = stream.map(lambda x: x[1])\n",
    "\n",
    "# Perform transformations on the data (for example, filtering)\n",
    "filtered_lines = lines.filter(lambda line: \"your_condition\" in line)\n",
    "\n",
    "# Output the processed data\n",
    "filtered_lines.pprint()\n",
    "\n",
    "# Start the streaming context\n",
    "ssc.start()\n",
    "\n",
    "# Wait for the streaming to finish\n",
    "ssc.awaitTermination()\n",
    "\n",
    "Scala\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import org.apache.spark.streaming.kafka010.{ConsumerStrategies, KafkaUtils, LocationStrategies}\n",
    "\n",
    "// Create a Spark configuration\n",
    "val conf = new SparkConf().setAppName(\"StreamingExample\")\n",
    "\n",
    "// Create a StreamingContext with a batch interval of 5 seconds\n",
    "val ssc = new StreamingContext(conf, Seconds(5))\n",
    "\n",
    "// Define Kafka parameters\n",
    "val kafkaParams = Map(\"bootstrap.servers\" -> \"your_kafka_broker\", \"key.deserializer\" -> \"org.apache.kafka.common.serialization.StringDeserializer\", \"value.deserializer\" -> \"org.apache.kafka.common.serialization.StringDeserializer\", \"group.id\" -> \"your_group_id\")\n",
    "\n",
    "// Create a DStream that represents the stream of data from Kafka (topic: \"your_topic\")\n",
    "val stream = KafkaUtils.createDirectStream[String, String](ssc, LocationStrategies.PreferConsistent, ConsumerStrategies.Subscribe[String, String](Set(\"your_topic\"), kafkaParams))\n",
    "\n",
    "// Process each micro-batch\n",
    "val lines = stream.map(record => record.value)\n",
    "\n",
    "// Perform transformations on the data (for example, filtering)\n",
    "val filteredLines = lines.filter(line => line.contains(\"your_condition\"))\n",
    "\n",
    "// Output the processed data\n",
    "filteredLines.print()\n",
    "\n",
    "// Start the streaming context\n",
    "ssc.start()\n",
    "\n",
    "// Wait for the streaming to finish\n",
    "ssc.awaitTermination()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b810ab-487a-47c6-b021-12c6c2432212",
   "metadata": {},
   "source": [
    "apply a transformation to the streaming data (e.g., filtering, aggregation):\n",
    "\n",
    "In the provided examples, the transformation is performed using the filter operation. You can replace it with any custom transformation logic based on your requirements.\n",
    "\n",
    "c. Output the processed data to a sink:\n",
    "\n",
    "In the provided examples, the processed data is printed using the pprint() operation. You can replace it with the appropriate sink operation based on your use case. For example, you can use foreachRDD to write the data to a file, a database, or any other destination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6d65dd-73be-4807-8b03-8bbb74cbfe06",
   "metadata": {},
   "source": [
    "11. Explain the fundamental concepts of Apache Kafka. What is it, and what problems does it aim to solve in\n",
    "the context of big data and real-time data processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6932c6-b818-4bb0-956b-dad7252f50dd",
   "metadata": {},
   "source": [
    "\n",
    "Apache Kafka:\n",
    "\n",
    "Definition:\n",
    "Apache Kafka is an open-source distributed streaming platform that is designed to handle real-time data feeds and provide a unified, high-throughput, fault-tolerant, and scalable platform for building real-time data pipelines and streaming applications. It was originally developed by LinkedIn and later open-sourced as an Apache Software Foundation project.\n",
    "\n",
    "Fundamental Concepts:\n",
    "\n",
    "Topics:\n",
    "\n",
    "Kafka organizes data into topics. A topic is a category or feed name to which messages are published by producers and from which messages are consumed by consumers.\n",
    "Partitions:\n",
    "\n",
    "Each topic is divided into partitions, and each partition can be hosted on a different server. Partitions allow Kafka to parallelize processing and scale horizontally.\n",
    "Brokers:\n",
    "\n",
    "Kafka runs as a cluster of servers, and each server in the cluster is called a broker. Brokers store data, serve client requests, and manage partitions.\n",
    "Producers:\n",
    "\n",
    "Producers are responsible for publishing messages to Kafka topics. They push data to Kafka brokers, and the data is then distributed across the topic's partitions.\n",
    "Consumers:\n",
    "\n",
    "Consumers subscribe to one or more topics and process the messages produced to those topics. Kafka consumers are designed to be scalable, fault-tolerant, and able to keep up with high data ingestion rates.\n",
    "Consumer Groups:\n",
    "\n",
    "Consumers can be organized into consumer groups, where each group receives a copy of the messages in a topic. This allows for parallel processing of messages by different consumer instances.\n",
    "Zookeeper:\n",
    "\n",
    "Zookeeper is a distributed coordination service used by Kafka for managing and coordinating the Kafka brokers. It helps in leader election, topic configuration management, and maintaining metadata.\n",
    "Offsets:\n",
    "\n",
    "Kafka keeps track of the position of a consumer in a partition using offsets. Each message in a partition is assigned a unique offset, and consumers keep track of the offset they have consumed up to.\n",
    "Problems Addressed by Kafka in Big Data and Real-time Data Processing:\n",
    "\n",
    "Scalability:\n",
    "\n",
    "Kafka provides horizontal scalability by allowing the distribution of data across multiple partitions and brokers. This enables it to handle large amounts of data and traffic.\n",
    "Durability:\n",
    "\n",
    "Kafka ensures data durability by persisting messages to disk and replicating them across multiple brokers. This prevents data loss even if a broker fails.\n",
    "Real-time Data Processing:\n",
    "\n",
    "Kafka is designed for low-latency, real-time data processing. It allows applications to subscribe to topics and process data as it arrives, enabling real-time analytics and monitoring.\n",
    "Fault Tolerance:\n",
    "\n",
    "Kafka is fault-tolerant by design. Data is replicated across multiple brokers, and if a broker fails, another broker can take over the responsibility, ensuring continuous availability.\n",
    "Decoupling of Producers and Consumers:\n",
    "\n",
    "Kafka acts as a decoupling mechanism between data producers and consumers. Producers can push data to Kafka without worrying about who consumes it, and consumers can process data without affecting the producers.\n",
    "Unified Platform for Event Streaming:\n",
    "\n",
    "Kafka provides a unified platform for handling event streaming, making it easier to build real-time data pipelines and streaming applications. It supports various use cases, including data integration, log aggregation, and real-time analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de7648c-04c2-4f96-91a0-5261f321fb79",
   "metadata": {},
   "source": [
    "12. Describe the architecture of Kafka, including its key components such as Producers, Topics, Brokers,\n",
    "Consumers, and ZooKeeper. How do these components work together in a Kafka cluster to achieve data\n",
    "streaming?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a0163-60d3-42b3-ae18-c5a51ea02331",
   "metadata": {},
   "source": [
    "The architecture of Apache Kafka is distributed and designed to handle large-scale, fault-tolerant, and real-time data streaming. It consists of several key components that work together to enable the seamless flow of data through the Kafka cluster:\n",
    "\n",
    "Producers:\n",
    "\n",
    "Responsibility: Producers are entities that publish messages to Kafka topics.\n",
    "Role in the Cluster: Producers push data to Kafka brokers, and they are responsible for specifying the topic to which the message should be published.\n",
    "Interaction: Producers interact directly with Kafka brokers to publish messages.\n",
    "Topics:\n",
    "\n",
    "Responsibility: Topics are logical channels or categories to which messages are published by producers and from which messages are consumed by consumers.\n",
    "Role in the Cluster: Topics help organize and categorize messages, allowing for parallel processing through partitions.\n",
    "Interaction: Producers specify the topic when publishing messages, and consumers subscribe to topics to consume messages.\n",
    "Brokers:\n",
    "\n",
    "Responsibility: Brokers are individual Kafka servers that store and manage data. A Kafka cluster consists of multiple brokers.\n",
    "Role in the Cluster: Brokers store topic partitions and serve client requests from producers and consumers. Each partition is hosted by one broker, and brokers coordinate with each other for data distribution and replication.\n",
    "Interaction: Producers push data to brokers, and consumers pull data from brokers. Brokers also replicate data to ensure fault tolerance.\n",
    "Consumers:\n",
    "\n",
    "Responsibility: Consumers are entities that subscribe to topics and process messages from Kafka.\n",
    "Role in the Cluster: Consumers read messages from partitions, process them, and keep track of the offset (position) in each partition.\n",
    "Interaction: Consumers pull data from brokers, and Kafka maintains the offset for each consumer, allowing them to keep track of their progress.\n",
    "Partitions:\n",
    "\n",
    "Responsibility: Partitions are the units of parallelism within a topic. Each partition is a log, and messages are appended to the end of the log.\n",
    "Role in the Cluster: Partitions allow Kafka to parallelize data processing across multiple brokers and consumers.\n",
    "Interaction: Producers publish messages to specific partitions, and consumers consume messages from specific partitions.\n",
    "ZooKeeper:\n",
    "\n",
    "Responsibility: ZooKeeper is used for coordination, configuration management, and maintaining metadata about the Kafka cluster.\n",
    "Role in the Cluster: ZooKeeper helps in leader election for partitions, keeps track of the broker and topic configurations, and manages consumer offsets.\n",
    "Interaction: Kafka brokers and clients interact with ZooKeeper to coordinate activities and maintain a consistent view of the Kafka cluster.\n",
    "How Components Work Together in a Kafka Cluster:\n",
    "\n",
    "Producer Interaction:\n",
    "\n",
    "Producers send messages to Kafka brokers, specifying the topic to which the messages should be published.\n",
    "Broker Responsibilities:\n",
    "\n",
    "Brokers receive messages from producers and store them in topic partitions.\n",
    "Brokers replicate data across multiple brokers for fault tolerance.\n",
    "Brokers coordinate with ZooKeeper for leader election, topic configuration, and other cluster management tasks.\n",
    "Consumer Interaction:\n",
    "\n",
    "Consumers subscribe to topics and pull messages from specific partitions.\n",
    "Consumers keep track of the offset (position) in each partition to resume processing from the last consumed message in case of failure.\n",
    "ZooKeeper Coordination:\n",
    "\n",
    "ZooKeeper helps in leader election for partitions. Each partition has one leader and multiple followers.\n",
    "ZooKeeper manages consumer group coordination, maintaining the offset for each consumer in a group.\n",
    "Scaling and Fault Tolerance:\n",
    "\n",
    "Kafka scales horizontally by adding more brokers and partitions to the cluster.\n",
    "Kafka achieves fault tolerance by replicating data across multiple brokers and ensuring that each partition has a leader and followers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9386299c-c8e5-4080-b56b-8d26c76082bf",
   "metadata": {},
   "source": [
    "13. Create a step-by-step guide on how to produce data to a Kafka topic using a programming language of\n",
    "your choice and then consume that data from the topic. Explain the role of Kafka producers and consumers\n",
    "in this process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75cfec-8f2b-419c-95da-b89476c5bfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 1: Install the confluent_kafka Library\n",
    "pip install confluent_kafka\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a04ca6-771a-482e-8f98-a6d40bb821a4",
   "metadata": {},
   "source": [
    "Step 2: Set Up a Kafka Cluster\n",
    "Make sure you have a running Kafka cluster. If you don't have Kafka installed, you can follow the official Apache Kafka Quickstart guide.\n",
    "\n",
    "Step 3: Create a Kafka Topic\n",
    "Create a Kafka topic named \"example_topic.\" You can do this using the Kafka command-line tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcb79e9-f28d-4541-912c-b28b57818bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka-topics.sh --create --topic example_topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff920b3-2702-4174-906a-c706a1183c0e",
   "metadata": {},
   "source": [
    "Step 4: Produce Data to the Kafka Topic (Producer)\n",
    "Create a Python script (e.g., kafka_producer.py) to produce data to the Kafka topic. This script will act as the Kafka producer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60defa7c-9bcd-4cad-911c-4a23eb2cf5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f'Message delivery failed: {err}')\n",
    "    else:\n",
    "        print(f'Message delivered to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}')\n",
    "\n",
    "# Kafka broker configuration\n",
    "conf = {'bootstrap.servers': 'localhost:9092'}\n",
    "\n",
    "# Create a Kafka producer instance\n",
    "producer = Producer(conf)\n",
    "\n",
    "# Produce messages to the Kafka topic\n",
    "for i in range(5):\n",
    "    message = f'Message {i}'\n",
    "    producer.produce('example_topic', value=message, callback=delivery_report)\n",
    "\n",
    "# Wait for any outstanding messages to be delivered and delivery reports to be received\n",
    "producer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfba9e0-4103-4d73-a0fa-06ed52d62ab6",
   "metadata": {},
   "source": [
    "Step 5: Consume Data from the Kafka Topic (Consumer)\n",
    "Create another Python script (e.g., kafka_consumer.py) to consume data from the Kafka topic. This script will act as the Kafka consumer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010ceed5-653d-4477-afb4-fbc5128539d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "# Kafka broker configuration\n",
    "conf = {'bootstrap.servers': 'localhost:9092', 'group.id': 'my_consumer_group', 'auto.offset.reset': 'earliest'}\n",
    "\n",
    "# Create a Kafka consumer instance\n",
    "consumer = Consumer(conf)\n",
    "\n",
    "# Subscribe to the Kafka topic\n",
    "consumer.subscribe(['example_topic'])\n",
    "\n",
    "# Consume messages from the Kafka topic\n",
    "while True:\n",
    "    msg = consumer.poll(1.0)\n",
    "\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "            # End of partition event, not an error\n",
    "            continue\n",
    "        else:\n",
    "            print(f'Error: {msg.error()}')\n",
    "            break\n",
    "\n",
    "    print(f'Received message: {msg.value().decode(\"utf-8\")}')\n",
    "\n",
    "# Close the consumer\n",
    "consumer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7774c2a2-cc76-44a0-b7e7-49f77727cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "Step 6: Run the Scripts\n",
    "Run the producer script (kafka_producer.py) to produce messages to the Kafka topic:\n",
    " python kafka_producer.py\n",
    "python kafka_consumer.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c9687-fcfd-4278-97b0-2886045e5ab0",
   "metadata": {},
   "source": [
    "14. Discuss the importance of data retention and data partitioning in Kafka. How can these features be\n",
    "configured, and what are the implications for data storage and processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f061dc-66fb-4b45-b521-092768ccc154",
   "metadata": {},
   "source": [
    "Importance:\n",
    "Data retention in Kafka refers to the duration for which messages are stored in a topic before they are eligible for deletion. It is a crucial aspect of Kafka's design for several reasons:\n",
    "\n",
    "Historical Analysis: Data retention allows for historical analysis of data, enabling consumers to replay or process past messages.\n",
    "\n",
    "Reprocessing: In scenarios where data processing logic changes or errors need correction, having a retention period allows reprocessing of historical data.\n",
    "\n",
    "Regulatory Compliance: Many industries and applications have regulatory requirements specifying how long data must be retained. Kafka's data retention policies help meet such compliance needs.\n",
    "\n",
    "Configuration:\n",
    "Data retention in Kafka is configured at the topic level. When creating or altering a topic, you can set the retention.ms (retention in milliseconds) or retention.bytes (retention based on size) properties.\n",
    "\n",
    "Example using Kafka command-line tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01b2c3-9f65-4f88-8ac7-d2a55c80de74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set retention period to 7 days\n",
    "kafka-topics.sh --zookeeper localhost:2181 --alter --topic your_topic --config retention.ms=604800000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18241830-1077-4ff8-9100-d06f42bf9419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set default retention period for topics\n",
    "log.retention.ms=604800000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5e487-d05f-4e40-8ed2-0bdf5c599c7b",
   "metadata": {},
   "source": [
    "15. Give examples of real-world use cases where Apache Kafka is employed. Discuss why Kafka is the\n",
    "preferred choice in those scenarios, and what benefits it brings to the table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bd666f-4b68-44b9-8047-79301dd510b3",
   "metadata": {},
   "source": [
    "Streaming Analytics in Retail:\n",
    "\n",
    "Use Case:\n",
    "Retailers use Kafka for streaming analytics to process and analyze real-time data from various sources such as online transactions, in-store purchases, and inventory updates. This enables them to make informed decisions, optimize pricing strategies, and improve customer experiences.\n",
    "\n",
    "Why Kafka:\n",
    "\n",
    "Real-time Processing: Kafka's low-latency streaming capabilities allow retailers to analyze data as it flows in, providing real-time insights.\n",
    "Scalability: Retail operations often involve a large volume of transactions, and Kafka's scalability ensures that it can handle the high throughput of data.\n",
    "Fault Tolerance: Kafka's distributed architecture ensures fault tolerance, critical for continuous retail operations.\n",
    "Microservices Communication in Telecommunications:\n",
    "\n",
    "Use Case:\n",
    "Telecommunications companies leverage Kafka for inter-service communication in microservices architectures. Kafka acts as a distributed message bus, facilitating communication between different microservices handling tasks like billing, authentication, and network management.\n",
    "\n",
    "Why Kafka:\n",
    "\n",
    "Decoupling of Services: Kafka enables services to communicate without being directly aware of each other, promoting loose coupling in microservices architectures.\n",
    "Reliability: The publish-subscribe model of Kafka ensures reliable and fault-tolerant communication between services.\n",
    "Scalability: As the number of microservices grows, Kafka scales horizontally, accommodating the increased communication load.\n",
    "Fraud Detection in Financial Services:\n",
    "\n",
    "Use Case:\n",
    "Financial institutions use Kafka for real-time fraud detection. Kafka streams transaction data from various sources, allowing for the immediate identification of unusual patterns or suspicious activities that may indicate fraudulent transactions.\n",
    "\n",
    "Why Kafka:\n",
    "\n",
    "Real-time Processing: Kafka's ability to process streaming data in real-time enables quick detection of anomalies and fraudulent activities.\n",
    "Event Sourcing: Kafka's event-driven architecture supports event sourcing, recording and processing financial transactions as a series of events.\n",
    "Scalability: Kafka's scalability ensures that it can handle the high volume of financial transactions generated in real-time.\n",
    "Log Aggregation in Tech Companies:\n",
    "\n",
    "Use Case:\n",
    "Technology companies use Kafka for log aggregation, collecting and processing logs generated by various applications and services. Kafka provides a centralized platform for storing, analyzing, and monitoring logs, aiding in debugging and performance optimization.\n",
    "\n",
    "Why Kafka:\n",
    "\n",
    "Unified Log: Kafka acts as a unified log, collecting logs from diverse sources and providing a centralized view of system activities.\n",
    "Scalability: As the number of applications and services grows, Kafka scales horizontally to handle the increasing volume of log data.\n",
    "Integration: Kafka integrates well with different logging frameworks and systems.\n",
    "IoT Data Processing in Manufacturing:\n",
    "\n",
    "Use Case:\n",
    "Manufacturing companies utilize Kafka for processing data from IoT devices on the factory floor. Kafka streams data related to equipment status, production metrics, and sensor readings, enabling predictive maintenance and process optimization.\n",
    "\n",
    "Why Kafka:\n",
    "\n",
    "Real-time Processing: Kafka's ability to handle real-time streaming of IoT data ensures timely insights for operational decision-making.\n",
    "Scalability: As the number of IoT devices increases, Kafka scales horizontally to handle the growing volume of data.\n",
    "Integration: Kafka integrates seamlessly with various IoT platforms and protocols.\n",
    "Benefits of Kafka in these Use Cases:\n",
    "\n",
    "Scalability: Kafka's distributed architecture and partitioning enable horizontal scaling to handle growing data volumes and workloads.\n",
    "Fault Tolerance: Kafka's replication and distributed nature ensure fault tolerance, preventing data loss and ensuring system reliability.\n",
    "Real-time Processing: Kafka's low-latency characteristics make it suitable for scenarios that require real-time data processing and analytics.\n",
    "Integration: Kafka's compatibility with various systems and protocols allows it to seamlessly integrate into diverse technology stacks.\n",
    "Durability: Kafka's persistence mechanisms provide data durability, preventing data loss and ensuring the availability of historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56a5289-4c4d-4c97-954d-4e3b0f444617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca7278e-6811-4268-8f9d-87d445992456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
